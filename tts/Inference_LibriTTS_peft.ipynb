{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc629bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# SPDX-FileCopyrightText: Copyright Â© 2024 Idiap Research Institute <contact@idiap.ch>\n",
    "#\n",
    "# SPDX-FileContributor: Haolin Chen <haolin.chen@idiap.ch>\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb7bd1",
   "metadata": {},
   "source": [
    "# StyleTTS 2 Demo (LibriTTS)\n",
    "\n",
    "Before you run the following cells, please make sure you have downloaded [reference_audio.zip](https://drive.google.com/file/d/1YhQO4O4dAsvkMzWZM8nVFMglYyi554YT/view) and unzipped it under the `demo` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e173bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def compute_style(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, sr, 24000)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(\"Models/LibriTTS/config.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc18cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = recursive_munch(config['model_params'])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64529d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_whole = torch.load(\"Models/LibriTTS/epochs_2nd_00020.pth\", map_location='cpu')\n",
    "params = params_whole['net']\n",
    "\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42506ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT: Apply LoRA to Linear layers\n",
    "from peft import LoraConfig, inject_adapter_in_model\n",
    "from peft.tuners.lora.layer import LoraLayer\n",
    "\n",
    "\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_path = \"Models/p248_lora16a32/epoch_2nd_00049.pth\"\n",
    "\n",
    "# Inject LoRA adapters, excluding static models\n",
    "full_finetune_model_names = ['mpd', 'msd', 'wd', 'text_aligner']\n",
    "static_model_names = ['pitch_extractor', 'text_encoder']\n",
    "for model_name, m in model.items():\n",
    "    if model_name in static_model_names + full_finetune_model_names:\n",
    "        continue\n",
    "    target_modules = []\n",
    "    for k, v in m.named_modules():\n",
    "        if isinstance(v, torch.nn.modules.linear.Linear):\n",
    "            target_modules.append(k)\n",
    "    if target_modules:\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "        )\n",
    "        model[model_name] = inject_adapter_in_model(lora_config, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbadb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Record LoRA modules\n",
    "modules_to_adapt = {}\n",
    "for model_name, m in model.items():\n",
    "    for k, v in m.named_modules():\n",
    "        if isinstance(v, LoraLayer):\n",
    "            modules_to_adapt[f'{model_name}:{k}'] = v\n",
    "\n",
    "model_state_dict = torch.load(lora_path, map_location=torch.device(device))\n",
    "for model_name, state_dict in model_state_dict.items():\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if model_name == 'bert_encoder':\n",
    "            name = k[6:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "        else:\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "    # load params\n",
    "    model[model_name].load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "_ = [model[key].eval() for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a59db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30985ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
    "    clamp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {},
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, ref_s, alpha = 0.3, beta = 0.7, diffusion_steps=5, embedding_scale=1):\n",
    "    text = text.strip()\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    ps = word_tokenize(ps[0])\n",
    "    ps = ' '.join(ps)\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2) \n",
    "\n",
    "        s_pred = sampler(noise = torch.randn((1, 256)).unsqueeze(1).to(device), \n",
    "                                          embedding=bert_dur,\n",
    "                                          embedding_scale=embedding_scale,\n",
    "                                            features=ref_s, # reference from the same speaker as the embedding\n",
    "                                             num_steps=diffusion_steps).squeeze(1)\n",
    "\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha)  * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta)  * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, \n",
    "                                         s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, \n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "    \n",
    "        \n",
    "    return out.squeeze().cpu().numpy()[..., :-50] # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438ef4f",
   "metadata": {},
   "source": [
    "#### Basic synthesis (5 diffusion steps, seen speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "reference_dicts['696_92939'] = \"Demo/reference_audio/696_92939_000016_000006.wav\"\n",
    "reference_dicts['1789_142896'] = \"Demo/reference_audio/1789_142896_000022_000005.wav\"\n",
    "reference_dicts['p225'] = \"Demo/reference_audio/vctk/p225/p225_314.wav\"\n",
    "reference_dicts['p234'] = \"Demo/reference_audio/vctk/p234/p234_174.wav\"\n",
    "reference_dicts['p238'] = \"Demo/reference_audio/vctk/p238/p238_392.wav\"\n",
    "reference_dicts['p245'] = \"Demo/reference_audio/vctk/p245/p245_012.wav\"\n",
    "reference_dicts['p248'] = \"Demo/reference_audio/vctk/p248/p248_023_00000.wav\"\n",
    "reference_dicts['p261'] = \"Demo/reference_audio/vctk/p261/p261_196.wav\"\n",
    "reference_dicts['p294'] = \"Demo/reference_audio/vctk/p294/p294_017.wav\"\n",
    "reference_dicts['p302'] = \"Demo/reference_audio/vctk/p302/p302_250.wav\"\n",
    "reference_dicts['p326'] = \"Demo/reference_audio/vctk/p326/p326_390.wav\"\n",
    "reference_dicts['p335'] = \"Demo/reference_audio/vctk/p335/p335_300.wav\"\n",
    "reference_dicts['p347'] = \"Demo/reference_audio/vctk/p347/p347_016.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "noise = torch.randn(1,1,256).to(device)\n",
    "for k, path in reference_dicts.items():\n",
    "    ref_s = compute_style(path)\n",
    "    \n",
    "    wav = inference(text, ref_s, alpha=0.3, beta=0.7, diffusion_steps=5, embedding_scale=1)\n",
    "    rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "    print(f\"RTF = {rtf:5f}\")\n",
    "    import IPython.display as ipd\n",
    "    print(k + ' Synthesized:')\n",
    "    display(ipd.Audio(wav, rate=24000, normalize=False))\n",
    "    print('Reference:')\n",
    "    display(ipd.Audio(path, rate=24000, normalize=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
